
import os, sys, boto3
import base64
from variables import KNOWLEDGE_BASE_ID, FM_GENERATION_MODEL_ARN, MODEL_ARN, REGION

bedrock_agent_runtime_client = boto3.client("bedrock-agent-runtime", region_name=REGION)

# def query_knowledge_base(question):
def retrieve_context_from_kb(question):
    """Retriever: Knowledge Baseì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰"""
    response = bedrock_agent_runtime_client.retrieve_and_generate(
        input={
            'text': question
        },
        retrieveAndGenerateConfiguration={
            'type': 'KNOWLEDGE_BASE',
            'knowledgeBaseConfiguration': {
                'knowledgeBaseId': KNOWLEDGE_BASE_ID,
                'modelArn': MODEL_ARN,
                'retrievalConfiguration': {
                    'vectorSearchConfiguration': {
                        'overrideSearchType': 'HYBRID'
                    }
                }
            }
        },
    )
    # return response['output']['text']
    return response['citations'], response['output']['text']  # ë¬¸ë§¥ + ì´ˆê¸° ì´ˆì•ˆ

# def query_foundation_model(prompts):
#     """
#     Step 2: Just call FM. (ì´ì „ code for type=MODEL)
#     'prompts' is the combined input or single string
#     """
def generate_answer_with_context(prompt):
    """Generator: ë¬¸ë§¥ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±"""
    response = bedrock_agent_runtime_client.retrieve_and_generate(
        # input={ 'text': prompts },
        input={ 'text': prompt },
        retrieveAndGenerateConfiguration={
            'type': 'EXTERNAL_SOURCES',
            'externalSourcesConfiguration': {
                'modelArn': FM_GENERATION_MODEL_ARN,  # e.g. anthropic.claude, ai21, TitanText
                'sources': [
                    {
                        'sourceType': 'BYTE_CONTENT',
                        'byteContent':{
                            'data': base64.b64encode(prompt.encode()).decode('utf-8'),
                            'contentType': 'text/plain',
                            'identifier': 'context-1'
                            }
                        }
                    ]
            }
        },
    )
    return response['output']['text']

def query_rag_plus_fm(question):
    # """
    # Hybrid approach:
    # 1) KB-based RAG => initial answer
    # 2) Then feed that answer (along with user Q) to a foundation model => refined final answer
    # """
    # # Step 1: retrieve from KB
    # rag_answer = query_knowledge_base(question)
    
    # # Compose final prompt for FM
    """Retriever + Generator í•˜ì´ë¸Œë¦¬ë“œ RAG êµ¬ì¡°"""
    citations, rag_answer = retrieve_context_from_kb(question)

    final_prompt = f"""
<system>
You are AWS Bedrock RAG assistant. 
â€¢ Write in concise Korean.  
â€¢ Never invent information that is not in the context.  
â€¢ Cite sources like [1], [2] right after the sentence that uses them.  
â€¢ Return **Markd
own** with at most three H2 sections: ğŸ“Answer / ğŸ”—Sources / ğŸ’¡Follow-up.
</system>

<user_question>
{question}
</user_question>

<context>
{rag_answer}
</context>

<task>
Step 1 â€“ Read <context>.  
Step 2 â€“ Produce the ğŸ“Answer section (â‰¤ 200 tokens).  
Step 3 â€“ List unique citations you actually used under ğŸ”—Sources.  
Step 4 â€“ Suggest one related question under ğŸ’¡Follow-up.
</task>
"""
    # Step 2: FM refine
    # final_answer = query_foundation_model(final_prompt)
    
    final_prompt = build_final_prompt(question, rag_answer, citations)   # helperë¡œ ë¶„ë¦¬
    return generate_answer_with_context(final_prompt)